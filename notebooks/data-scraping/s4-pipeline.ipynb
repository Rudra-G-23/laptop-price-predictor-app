{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660d63ea",
   "metadata": {},
   "source": [
    "# Logic\n",
    "\n",
    "1. Start from **page 1**\n",
    "2. Fetch the **search result page URL**\n",
    "3. Extract **all product links on that page**\n",
    "4. Visit **each product link** â†’ scrape details\n",
    "5. Save results to **CSV for that page**\n",
    "\n",
    "   * `amazon_laptops_page_1.csv`\n",
    "6. Move to **page 2**\n",
    "7. Repeat until page N\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c03fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from curl_cffi import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f48cc2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"Sec-Ch-Ua\": '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"',\n",
    "    \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "    \"Sec-Ch-Ua-Platform\": '\"Windows\"',\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-Site\": \"cross-site\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "408ac1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_page(page_no):\n",
    "    \"\"\"Fetch Amazon search result page\"\"\"\n",
    "    url = f\"https://www.amazon.in/s?k=laptop&page={page_no}\"\n",
    "    response = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    return BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94ae0b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_page_cards(soup):\n",
    "    \"\"\"How many cards available in that page\"\"\"\n",
    "    cards = soup.find_all(\"div\",\n",
    "    class_=\"a-section a-spacing-small a-spacing-top-small\"\n",
    "    )\n",
    "    return len(cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb3c413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_links(soup):\n",
    "    \"\"\"Full link of the product\"\"\"\n",
    "    links = []\n",
    "    tags = soup.select(\n",
    "        \"a.a-link-normal.s-line-clamp-2.s-link-style.a-text-normal\"\n",
    "    )\n",
    "\n",
    "    for tag in tags:\n",
    "        href = tag.get(\"href\")\n",
    "        if href:\n",
    "            links.append(\"https://www.amazon.in\" + href)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66324174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_text(tag, default=None):\n",
    "    \"\"\"Safely extract text from a BeautifulSoup tag\"\"\"\n",
    "    return tag.get_text(strip=True) if tag else default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "445a7409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_PAGES = 2\n",
    "\n",
    "session = requests.Session()\n",
    "session = requests.Session()\n",
    "session.get(\n",
    "    \"https://www.amazon.in\",\n",
    "    headers=HEADERS,\n",
    "    impersonate=\"chrome120\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "226d6414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_details(new_soup):\n",
    "    \"\"\"Help to Clean the table data\"\"\"\n",
    "    \n",
    "    product_table = new_soup.find(\"table\", class_=\"a-normal a-spacing-micro\")\n",
    "\n",
    "    product_details = {}\n",
    "\n",
    "    if product_table:\n",
    "        rows = product_table.find_all(\"tr\")\n",
    "        \n",
    "        for row in rows:\n",
    "            cols = row.find_all(\"td\")\n",
    "            \n",
    "            if len(cols)!= 2:\n",
    "                continue\n",
    "            \n",
    "            key = cols[0].get_text(strip=True)\n",
    "            value = cols[1].get_text(strip=True)\n",
    "            \n",
    "            product_details[key] = value\n",
    "            \n",
    "    return product_details\n",
    "\n",
    "def extract_technical_details(new_soup):\n",
    "    \"\"\"More technical stuff about the laptop\"\"\"\n",
    "    \n",
    "    technical_table = new_soup.find(\n",
    "        \"table\",\n",
    "        id=\"productDetails_techSpec_section_1\"\n",
    "    )\n",
    "\n",
    "    technical_details = {}\n",
    "\n",
    "    if technical_table:\n",
    "        rows = technical_table.find_all(\"tr\")\n",
    "        \n",
    "        for row in rows:\n",
    "            key_tag = row.find(\"th\")\n",
    "            value_tag = row.find(\"td\")\n",
    "            \n",
    "            if not key_tag or not value_tag:\n",
    "                continue\n",
    "            \n",
    "            key = key_tag.get_text(strip=True)\n",
    "            value = value_tag.get_text(strip=True)\n",
    "            \n",
    "            technical_details[key] = value\n",
    "            \n",
    "    return technical_details\n",
    "\n",
    "\n",
    "def fetch_url_data(url):\n",
    "    \"\"\"Fetch each url\"\"\"\n",
    "    #url = extract_product_links(soup)\n",
    "    new_response = requests.get(url, headers=HEADERS, timeout=20)\n",
    "    new_soup = BeautifulSoup(new_response.text, \"html.parser\")\n",
    "    \n",
    "    return new_soup\n",
    "\n",
    "def fetch_data(new_soup):\n",
    "    \"\"\"Fetch the data form the each page\"\"\"\n",
    "    \n",
    "    product_details= extract_product_details(new_soup)\n",
    "    technical_details= extract_technical_details(new_soup)\n",
    "   \n",
    "    data = {}\n",
    "    \n",
    "    # --- Product title, Ratings ---\n",
    "    data[\"product_title\"] = safe_text(new_soup.find(\"span\", attrs={'id':\"productTitle\"}))\n",
    "    data[\"rating\"] = safe_text(new_soup.find(\"span\", class_=\"a-size-small a-color-base\"))\n",
    "    data[\"total_rating\"] = safe_text(new_soup.find(\"span\", class_=\"a-size-small\", attrs={'id':\"acrCustomerReviewText\"}))\n",
    "    \n",
    "    # --- Pricing ----\n",
    "    data[\"price\"] = safe_text(new_soup.find(\"span\", class_=\"a-price-whole\"))\n",
    "    \n",
    "    data[\"next_month_bought\"] = safe_text(new_soup.find(\n",
    "        \"span\",\n",
    "        id='social-proofing-faceout-title-tk_bought',\n",
    "        class_=\"a-size-small social-proofing-faceout-title-text social-proofing-faceout-cx-enhancement-T2\"\n",
    "    ))\n",
    "    \n",
    "    # --- offer, mrp ---\n",
    "    data[\"offer\"] = safe_text(new_soup.find(\n",
    "      \"span\"  ,\n",
    "      class_=\"a-size-large a-color-price savingPriceOverride aok-align-center reinventPriceSavingsPercentageMargin savingsPercentage\"\n",
    "    ))\n",
    "    \n",
    "    data[\"mrp\"] = safe_text(new_soup.find(\n",
    "      \"span\"  ,\n",
    "      class_=\"a-price aok-align-center reinventPricePriceToPayMargin priceToPay\"\n",
    "    ))\n",
    "    \n",
    "    # --- Product Details ---\n",
    "    data[\"product_details_dict\"] = product_details\n",
    "    data[\"brand\"] = product_details.get(\"Brand\")\n",
    "    data[\"model_name\"]= product_details.get(\"Model Name\")\n",
    "    data[\"screen_size\"]= product_details.get(\"Screen Size\")\n",
    "    data[\"colour\"]= product_details.get(\"Colour\")\n",
    "    data[\"hard_disk_size\"]= product_details.get(\"Hard Disk Size\")\n",
    "    data[\"cpu_model\"]= product_details.get(\"CPU Model\")\n",
    "    data[\"ram\"]= product_details.get(\"RAM Memory Installed Size\")\n",
    "    data[\"os\"]= product_details.get(\"Operating System\")\n",
    "    data[\"special_features\"]= product_details.get(\"Special Feature\")\n",
    "    data[\"graphics_card\"]= product_details.get(\"Graphics Card Description\")\n",
    "    \n",
    "    # --- about the item ---\n",
    "    data[\"about\"] = safe_text(new_soup.find(\"div\", class_=\"a-section a-spacing-medium a-spacing-top-small\"))\n",
    "    data[\"technical_details_dict\"] = technical_details\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb705e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping page 1\n",
      "    Product: 1/22\n",
      "    Product: 2/22\n",
      "    Product: 3/22\n",
      "    Product: 4/22\n",
      "    Product: 5/22\n",
      "    Product: 6/22\n",
      "    Product: 7/22\n",
      "    Product: 8/22\n",
      "    Product: 9/22\n",
      "    Product: 10/22\n",
      "    Product: 11/22\n",
      "    Product: 12/22\n",
      "    Product: 13/22\n",
      "    Product: 14/22\n",
      "    Product: 15/22\n",
      "    Product: 16/22\n",
      "    Product: 17/22\n",
      "    Product: 18/22\n",
      "    Product: 19/22\n",
      "    Product: 20/22\n",
      "    Product: 21/22\n",
      "    Product: 22/22\n",
      "\n",
      " ----- Saved data/amazon_laptop_1.csv -----\n",
      "Scrapping page 2\n",
      "    Product: 1/22\n",
      "    Product: 2/22\n",
      "    Product: 3/22\n",
      "    Product: 4/22\n",
      "    Product: 5/22\n",
      "    Product: 6/22\n",
      "    Product: 7/22\n",
      "    Product: 8/22\n",
      "    Product: 9/22\n",
      "    Product: 10/22\n",
      "    Product: 11/22\n",
      "    Product: 12/22\n",
      "    Product: 13/22\n",
      "    Product: 14/22\n",
      "    Product: 15/22\n",
      "    Product: 16/22\n",
      "    Product: 17/22\n",
      "    Product: 18/22\n",
      "    Product: 19/22\n",
      "    Product: 20/22\n",
      "    Product: 21/22\n",
      "    Product: 22/22\n",
      "\n",
      " ----- Saved data/amazon_laptop_2.csv -----\n",
      "\n",
      " Scraping completed!\n"
     ]
    }
   ],
   "source": [
    "MAX_PAGES = 2\n",
    "\n",
    "for page in range(1, MAX_PAGES +1):\n",
    "    print(f\"Scrapping page {page}\")\n",
    "    \n",
    "    search_soup = get_search_page(page_no=page)\n",
    "    count_page_cards(search_soup)\n",
    "    product_links = extract_product_links(search_soup)\n",
    "    \n",
    "    page_data = []\n",
    "    \n",
    "    for idx, link in enumerate(product_links, start=1):\n",
    "        print(f\"    Product: {idx}/{len(product_links)}\")\n",
    "        try:\n",
    "            product_soup = fetch_url_data(link)\n",
    "            data = fetch_data(product_soup)\n",
    "            page_data.append(data)\n",
    "            \n",
    "            time.sleep(random.uniform(3, 10))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error: {e}\")\n",
    "    \n",
    "    # Save the files\n",
    "    df = pd.DataFrame(page_data)\n",
    "    filename = f\"data/amazon_laptop_{page}.csv\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"\\n ----- Saved {filename} -----\\n\")\n",
    "\n",
    "print(\"\\n Scraping completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laptop-price-predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
